{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.0.0.tar.gz (204.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 204.7 MB 7.0 MB/s eta 0:00:012   |█▊                              | 11.3 MB 4.7 MB/s eta 0:00:42�████████▍                  | 85.5 MB 1.6 MB/s eta 0:01:15     |██████████████████▍             | 117.3 MB 44.3 MB/s eta 0:00:02     |███████████████████▏            | 122.6 MB 1.9 MB/s eta 0:00:43     |████████████████████▊           | 132.8 MB 8.1 MB/s eta 0:00:09     |████████████████████████████▏   | 179.9 MB 2.4 MB/s eta 0:00:11     |████████████████████████████▍   | 181.5 MB 2.4 MB/s eta 0:00:10     |█████████████████████████████   | 185.7 MB 5.5 MB/s eta 0:00:04     |██████████████████████████████▉ | 197.5 MB 2.6 MB/s eta 0:00:03\n",
      "\u001b[?25hCollecting py4j==0.10.9\n",
      "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.0.0-py2.py3-none-any.whl size=205044182 sha256=44a533f1abdeff8244e74bde4b158fb1f893d1940c72484deac9636ac8d88b87\n",
      "  Stored in directory: /Users/yan/Library/Caches/pip/wheels/4e/c5/36/aef1bb711963a619063119cc032176106827a129c0be20e301\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9 pyspark-3.0.0\n"
     ]
    }
   ],
   "source": [
    "# Install PySpark\n",
    "!pip install -q findspark\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile sparkSpam.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import RegexTokenizer, Tokenizer, CountVectorizer\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "APP_NAME = \"spamFilter\"\n",
    "\n",
    "spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
    "\n",
    "spark\n",
    "\n",
    "file_path = './data/spam.csv'\n",
    "\n",
    "data = spark.read.format('csv').option('header', 'true').option('inferSchema', 'true').load(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----+----+----+\n",
      "|  v1|                  v2| _c2| _c3| _c4|\n",
      "+----+--------------------+----+----+----+\n",
      "| ham|Go until jurong p...|null|null|null|\n",
      "| ham|Ok lar... Joking ...|null|null|null|\n",
      "|spam|Free entry in 2 a...|null|null|null|\n",
      "| ham|U dun say so earl...|null|null|null|\n",
      "| ham|Nah I don't think...|null|null|null|\n",
      "|spam|FreeMsg Hey there...|null|null|null|\n",
      "| ham|Even my brother i...|null|null|null|\n",
      "| ham|As per your reque...|null|null|null|\n",
      "|spam|WINNER!! As a val...|null|null|null|\n",
      "|spam|Had your mobile 1...|null|null|null|\n",
      "| ham|I'm gonna be home...|null|null|null|\n",
      "|spam|SIX chances to wi...|null|null|null|\n",
      "|spam|URGENT! You have ...|null|null|null|\n",
      "| ham|I've been searchi...|null|null|null|\n",
      "| ham|I HAVE A DATE ON ...|null|null|null|\n",
      "|spam|XXXMobileMovieClu...|null|null|null|\n",
      "| ham|Oh k...i'm watchi...|null|null|null|\n",
      "| ham|Eh u remember how...|null|null|null|\n",
      "| ham|Fine if that��s t...|null|null|null|\n",
      "|spam|England v Macedon...|null|null|null|\n",
      "+----+--------------------+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing columns and keep only v1 and v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep only v1 and v2\n",
      "+----+--------------------+\n",
      "|  v1|                  v2|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "|spam|Free entry in 2 a...|\n",
      "| ham|U dun say so earl...|\n",
      "| ham|Nah I don't think...|\n",
      "|spam|FreeMsg Hey there...|\n",
      "| ham|Even my brother i...|\n",
      "| ham|As per your reque...|\n",
      "|spam|WINNER!! As a val...|\n",
      "|spam|Had your mobile 1...|\n",
      "| ham|I'm gonna be home...|\n",
      "|spam|SIX chances to wi...|\n",
      "|spam|URGENT! You have ...|\n",
      "| ham|I've been searchi...|\n",
      "| ham|I HAVE A DATE ON ...|\n",
      "|spam|XXXMobileMovieClu...|\n",
      "| ham|Oh k...i'm watchi...|\n",
      "| ham|Eh u remember how...|\n",
      "| ham|Fine if that��s t...|\n",
      "|spam|England v Macedon...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# keep only important columns\n",
    "df = data.drop('_c1','_c2','_c3','_c4')\n",
    "print('keep only v1 and v2')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking unicue values of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking unique values in label\n",
      "+------+-----+\n",
      "|    v1|count|\n",
      "+------+-----+\n",
      "|ham\"\"\"|    2|\n",
      "|   ham| 4825|\n",
      "|  spam|  747|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('checking unique values in label')\n",
    "df.groupby('v1').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remving row with label ham\"\"\"\"\n",
    "\n",
    "As we can see that there are 2 rows that the label is ham\"\"\", since there are only two instances, we decided to remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing ham\"\"\"\n",
      "+----+-----+\n",
      "|  v1|count|\n",
      "+----+-----+\n",
      "| ham| 4825|\n",
      "|spam|  747|\n",
      "+----+-----+\n",
      "\n",
      "class priori probability\n",
      "spam: 0.13406317300789664 \n",
      "ham:0.8659368269921034\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('data')\n",
    "\n",
    "df = spark.sql(\"SELECT * FROM data WHERE v1 = 'ham' OR v1 = 'spam'\")\n",
    "print('Removing ham\"\"\"')\n",
    "group = df.groupby('v1').count()\n",
    "group.show()\n",
    "ham = group.collect()[0][1]\n",
    "spam = group.collect()[1][1]\n",
    "print('class priori probability')\n",
    "print(f'spam: {spam/(ham+spam)} \\nham:{ham/(spam+ham)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing lables to 0 and 1\n",
    "We index labels which ham = 0 and spam = 1. Then assign to a new column name label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexing label to 1 and 0\n",
      "+----+--------------------+-----+\n",
      "|  v1|                  v2|label|\n",
      "+----+--------------------+-----+\n",
      "| ham|Go until jurong p...|  0.0|\n",
      "| ham|Ok lar... Joking ...|  0.0|\n",
      "|spam|Free entry in 2 a...|  1.0|\n",
      "| ham|U dun say so earl...|  0.0|\n",
      "| ham|Nah I don't think...|  0.0|\n",
      "|spam|FreeMsg Hey there...|  1.0|\n",
      "| ham|Even my brother i...|  0.0|\n",
      "| ham|As per your reque...|  0.0|\n",
      "|spam|WINNER!! As a val...|  1.0|\n",
      "|spam|Had your mobile 1...|  1.0|\n",
      "+----+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('indexing label to 1 and 0')\n",
    "# indexing label to ham = 0 and  spam = 1 and put in column name label\n",
    "indexer = StringIndexer(inputCol = 'v1', outputCol = 'label')\n",
    "index_label = indexer.fit(df)\n",
    "\n",
    "df = index_label.transform(df)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize documents\n",
    "Tokenize columb v2 and assign to tokens column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toknize documents and assign to \"tokens\" column\n",
      "+----+--------------------+-----+--------------------+\n",
      "|  v1|                  v2|label|              tokens|\n",
      "+----+--------------------+-----+--------------------+\n",
      "| ham|Go until jurong p...|  0.0|[go, until, juron...|\n",
      "| ham|Ok lar... Joking ...|  0.0|[ok, lar..., joki...|\n",
      "|spam|Free entry in 2 a...|  1.0|[free, entry, in,...|\n",
      "| ham|U dun say so earl...|  0.0|[u, dun, say, so,...|\n",
      "| ham|Nah I don't think...|  0.0|[nah, i, don't, t...|\n",
      "|spam|FreeMsg Hey there...|  1.0|[freemsg, hey, th...|\n",
      "| ham|Even my brother i...|  0.0|[even, my, brothe...|\n",
      "| ham|As per your reque...|  0.0|[as, per, your, r...|\n",
      "|spam|WINNER!! As a val...|  1.0|[winner!!, as, a,...|\n",
      "|spam|Had your mobile 1...|  1.0|[had, your, mobil...|\n",
      "+----+--------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('toknize documents and assign to \"tokens\" column')\n",
    "tokenizer = RegexTokenizer(inputCol=\"v2\", outputCol=\"tokens\", pattern=\" \")\n",
    "df = tokenizer.transform(df)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feature vectors from tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change token column to feature vectors as \"features\" column\n",
      "+----+--------------------+-----+--------------------+--------------------+\n",
      "|  v1|                  v2|label|              tokens|            features|\n",
      "+----+--------------------+-----+--------------------+--------------------+\n",
      "| ham|Go until jurong p...|  0.0|[go, until, juron...|(13539,[8,41,51,6...|\n",
      "| ham|Ok lar... Joking ...|  0.0|[ok, lar..., joki...|(13539,[5,73,401,...|\n",
      "|spam|Free entry in 2 a...|  1.0|[free, entry, in,...|(13539,[0,3,8,20,...|\n",
      "| ham|U dun say so earl...|  0.0|[u, dun, say, so,...|(13539,[5,21,59,1...|\n",
      "| ham|Nah I don't think...|  0.0|[nah, i, don't, t...|(13539,[0,1,65,88...|\n",
      "|spam|FreeMsg Hey there...|  1.0|[freemsg, hey, th...|(13539,[0,2,6,10,...|\n",
      "| ham|Even my brother i...|  0.0|[even, my, brothe...|(13539,[0,7,9,13,...|\n",
      "| ham|As per your reque...|  0.0|[as, per, your, r...|(13539,[0,10,11,4...|\n",
      "|spam|WINNER!! As a val...|  1.0|[winner!!, as, a,...|(13539,[0,2,3,14,...|\n",
      "|spam|Had your mobile 1...|  1.0|[had, your, mobil...|(13539,[0,4,5,10,...|\n",
      "| ham|I'm gonna be home...|  0.0|[i'm, gonna, be, ...|(13539,[0,1,6,30,...|\n",
      "|spam|SIX chances to wi...|  1.0|[six, chances, to...|(13539,[0,6,39,46...|\n",
      "|spam|URGENT! You have ...|  1.0|[urgent!, you, ha...|(13539,[0,2,3,4,8...|\n",
      "| ham|I've been searchi...|  0.0|[i've, been, sear...|(13539,[0,1,2,3,4...|\n",
      "| ham|I HAVE A DATE ON ...|  0.0|[i, have, a, date...|(13539,[1,3,14,16...|\n",
      "|spam|XXXMobileMovieClu...|  1.0|[xxxmobilemoviecl...|(13539,[0,4,8,11,...|\n",
      "| ham|Oh k...i'm watchi...|  0.0|[oh, k...i'm, wat...|(13539,[158,314,4...|\n",
      "| ham|Eh u remember how...|  0.0|[eh, u, remember,...|(13539,[1,5,20,45...|\n",
      "| ham|Fine if that��s t...|  0.0|[fine, if, that��...|(13539,[4,5,28,58...|\n",
      "|spam|England v Macedon...|  1.0|[england, v, mace...|(13539,[0,4,27,80...|\n",
      "+----+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('change token column to feature vectors as \"features\" column')\n",
    "#count and vectorize decoment\n",
    "vectorize = CountVectorizer(inputCol = 'tokens', outputCol='features', vocabSize=20_000)\n",
    "model_cv = vectorize.fit(df)\n",
    "df = model_cv.transform(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create naiveBayes model with different laplace smoothing factor\n",
    "Train model with different laplace smooting factor and see the effect of the factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "\t\tModel Evalutaion\n",
      "-------------------------------------------\n",
      "with Laplace smoothing = 0\n",
      "Accuracy : 0.9052173913043479\n",
      "Precision : 0.9423076923076923\n",
      "Recall : 0.3161290322580645\n",
      "F-measure :  0.47342995169082125\n",
      "Run Time:  0:00:01.335840 \n",
      "\n",
      "with Laplace smoothing = 0.25\n",
      "Accuracy : 0.9686956521739131\n",
      "Precision : 0.8287292817679558\n",
      "Recall : 0.967741935483871\n",
      "F-measure :  0.8928571428571429\n",
      "Run Time:  0:00:01.336856 \n",
      "\n",
      "with Laplace smoothing = 0.5\n",
      "Accuracy : 0.971304347826087\n",
      "Precision : 0.8426966292134831\n",
      "Recall : 0.967741935483871\n",
      "F-measure :  0.9009009009009008\n",
      "Run Time:  0:00:01.240171 \n",
      "\n",
      "with Laplace smoothing = 0.75\n",
      "Accuracy : 0.9730434782608696\n",
      "Precision : 0.8563218390804598\n",
      "Recall : 0.9612903225806452\n",
      "F-measure :  0.905775075987842\n",
      "Run Time:  0:00:01.122621 \n",
      "\n",
      "with Laplace smoothing = 1\n",
      "Accuracy : 0.9756521739130435\n",
      "Precision : 0.8713450292397661\n",
      "Recall : 0.9612903225806452\n",
      "F-measure :  0.9141104294478527\n",
      "Run Time:  0:00:01.068811 \n",
      "\n",
      "with Laplace smoothing = 2\n",
      "Accuracy : 0.9791304347826087\n",
      "Precision : 0.9171974522292994\n",
      "Recall : 0.9290322580645162\n",
      "F-measure :  0.9230769230769231\n",
      "Run Time:  0:00:01.101236 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#split data in to traintest\n",
    "\n",
    "train, test = df.select('features','label').randomSplit([0.8,0.2],seed = 0)\n",
    "\n",
    "laplace_smooth = [0,0.25,0.5,0.75,1,2]\n",
    "print('-------------------------------------------')\n",
    "print('\\t\\tModel Evalutaion')\n",
    "print('-------------------------------------------')\n",
    "for laplace in laplace_smooth:\n",
    "    start = start=datetime.now()\n",
    "    nb = NaiveBayes(featuresCol = 'features', labelCol = 'label',smoothing=laplace, modelType='multinomial')\n",
    "    nb_model = nb.fit(train)\n",
    "    prediction = nb_model.transform(test)\n",
    "    result = prediction.select('prediction','label').rdd.map(tuple)\n",
    "    metrics = MulticlassMetrics(result)\n",
    "    f_measure = (2 * metrics.recall(label=1) * metrics.precision(label=1)) / (metrics.recall(label=1) + metrics.precision(label=1))\n",
    "    print(f'with Laplace smoothing = {laplace}')\n",
    "    print(f'Accuracy : {metrics.accuracy}')\n",
    "    print(f'Precision : {metrics.precision(label=1)}')\n",
    "    print(f'Recall : {metrics.recall(label=1)}')\n",
    "    print('F-measure : ', f_measure )\n",
    "    print('Run Time: ', datetime.now()-start, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that without smoothing factor i.e. smotting = 0 the model does a bad job on recall. The reason that model yeild high accuracy with smooth = 0 is that the unblance of classes in the data set i.e. the class prior is 0.87 and 0.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
